{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPAMtmF1ByV8arXUtPZ1pS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeilMitra/2WD-ObstacleAvoidingRobot/blob/master/Daily_Mover_Adjusted.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithm"
      ],
      "metadata": {
        "id": "VUkomQef4Nm_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "AsSlQ-HiTPqP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "524c70ff-1bf3-4c84-8d34-94b0856c2ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.11/dist-packages (0.2.55)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.32.3)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.3.7)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.11/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.11/dist-packages (from yfinance) (3.17.9)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.13.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->yfinance) (2025.1.31)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy yfinance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import datetime\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar\n",
        "from pandas.tseries.offsets import CustomBusinessDay\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "h5eL5fQc4HKm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_top_100_sp500_by_market_cap():\n",
        "    \"\"\"\n",
        "    Get the top 100 S&P 500 stocks by market capitalization with rate limit handling.\n",
        "    Caches results to avoid repeated API calls.\n",
        "    \"\"\"\n",
        "    cache_file = \"sp500_top100_cache.csv\"\n",
        "\n",
        "    # Check if we have a recent cache file (less than 24 hours old)\n",
        "    if os.path.exists(cache_file) and (datetime.datetime.now() -\n",
        "            datetime.datetime.fromtimestamp(os.path.getmtime(cache_file))).total_seconds() < 86400:\n",
        "        print(f\"Loading top 100 stocks from cache file: {cache_file}\")\n",
        "        return pd.read_csv(cache_file)['Symbol'].tolist()\n",
        "\n",
        "    # Get S&P 500 tickers\n",
        "    sp500_url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
        "    sp500_table = pd.read_html(sp500_url)[0]\n",
        "    sp500_symbols = sp500_table['Symbol'].tolist()\n",
        "\n",
        "    # Get market cap for all symbols with rate limiting\n",
        "    market_caps = {}\n",
        "    for i, symbol in enumerate(sp500_symbols):\n",
        "        try:\n",
        "            # Add random delay between requests to avoid rate limiting\n",
        "            if i > 0 and i % 5 == 0:\n",
        "                delay = random.uniform(1.0, 3.0)\n",
        "                print(f\"Sleeping for {delay:.2f} seconds to avoid rate limiting...\")\n",
        "                time.sleep(delay)\n",
        "\n",
        "            ticker = yf.Ticker(symbol)\n",
        "            market_cap = ticker.info.get('marketCap', 0)\n",
        "            if market_cap:\n",
        "                market_caps[symbol] = market_cap\n",
        "                print(f\"[{i+1}/{len(sp500_symbols)}] {symbol}: ${market_cap:,}\")\n",
        "            else:\n",
        "                print(f\"[{i+1}/{len(sp500_symbols)}] {symbol}: No market cap data available\")\n",
        "        except Exception as e:\n",
        "            print(f\"[{i+1}/{len(sp500_symbols)}] Error fetching data for {symbol}: {e}\")\n",
        "            # If we hit a rate limit, pause for a longer time\n",
        "            if \"rate limit\" in str(e).lower():\n",
        "                pause_time = random.uniform(10.0, 15.0)\n",
        "                print(f\"Rate limit detected. Pausing for {pause_time:.2f} seconds...\")\n",
        "                time.sleep(pause_time)\n",
        "\n",
        "    # Sort by market cap and get top 100\n",
        "    sorted_by_market_cap = sorted(market_caps.items(), key=lambda x: x[1], reverse=True)\n",
        "    top_100 = [symbol for symbol, _ in sorted_by_market_cap[:100]]\n",
        "\n",
        "    # Save to cache file\n",
        "    pd.DataFrame({'Symbol': top_100}).to_csv(cache_file, index=False)\n",
        "    print(f\"Saved top 100 stocks to cache file: {cache_file}\")\n",
        "\n",
        "    return top_100\n",
        "\n",
        "def get_trading_date(date, calendar):\n",
        "    \"\"\"\n",
        "    Get the nearest trading date for a given date.\n",
        "    If date is a weekend or holiday, return the previous trading day.\n",
        "    \"\"\"\n",
        "    us_bd = CustomBusinessDay(calendar=calendar)\n",
        "    dt = pd.Timestamp(date)\n",
        "\n",
        "    # If it's a business day, return the date\n",
        "    if dt in calendar:\n",
        "        return dt\n",
        "\n",
        "    # Otherwise, return the previous business day\n",
        "    return dt - us_bd\n",
        "\n",
        "def analyze_date_patterns(tickers, years=10):\n",
        "    \"\"\"\n",
        "    Analyze date patterns for a list of ticker symbols over a specified number of years.\n",
        "    Implements caching and rate limit handling.\n",
        "\n",
        "    Parameters:\n",
        "    tickers (list): List of ticker symbols\n",
        "    years (int): Number of years to analyze\n",
        "\n",
        "    Returns:\n",
        "    tuple: (up_patterns, down_patterns) where each is a dictionary mapping dates to stocks\n",
        "    \"\"\"\n",
        "    # Create directory for cached data\n",
        "    cache_dir = \"stock_data_cache\"\n",
        "    if not os.path.exists(cache_dir):\n",
        "        os.makedirs(cache_dir)\n",
        "\n",
        "    # Create a calendar for US trading days\n",
        "    cal = USFederalHolidayCalendar()\n",
        "    holidays = cal.holidays(start=datetime.datetime.now() - datetime.timedelta(days=years*365),\n",
        "                           end=datetime.datetime.now())\n",
        "\n",
        "    # Initialize pattern dictionaries\n",
        "    up_patterns = {}  # date -> [symbols that consistently go up]\n",
        "    down_patterns = {}  # date -> [symbols that consistently go down]\n",
        "\n",
        "    # Get current date\n",
        "    now = datetime.datetime.now()\n",
        "\n",
        "    # For each ticker, get historical data and analyze patterns\n",
        "    for i, ticker in enumerate(tickers):\n",
        "        print(f\"[{i+1}/{len(tickers)}] Analyzing {ticker}...\")\n",
        "\n",
        "        try:\n",
        "            # Define cache file path\n",
        "            cache_file = os.path.join(cache_dir, f\"{ticker}_data.csv\")\n",
        "\n",
        "            # Check if we have cached data and it's recent (less than 24 hours old)\n",
        "            if os.path.exists(cache_file) and (datetime.datetime.now() -\n",
        "                    datetime.datetime.fromtimestamp(os.path.getmtime(cache_file))).total_seconds() < 86400:\n",
        "                print(f\"  Loading cached data for {ticker}\")\n",
        "                historical_data = pd.read_csv(cache_file, index_col=0, parse_dates=True)\n",
        "            else:\n",
        "                # Implement rate limiting - add delay between batches of requests\n",
        "                if i > 0 and i % 3 == 0:\n",
        "                    delay = random.uniform(2.0, 5.0)\n",
        "                    print(f\"  Sleeping for {delay:.2f} seconds to avoid rate limiting...\")\n",
        "                    time.sleep(delay)\n",
        "\n",
        "                # Get historical data for the past 'years' years\n",
        "                start_date = (now - datetime.timedelta(days=years*365)).strftime('%Y-%m-%d')\n",
        "                print(f\"  Downloading data for {ticker} from {start_date} to now...\")\n",
        "\n",
        "                # Try with exponential backoff for rate limiting\n",
        "                max_retries = 5\n",
        "                retry_count = 0\n",
        "                backoff_time = 1\n",
        "\n",
        "                while retry_count < max_retries:\n",
        "                    try:\n",
        "                        historical_data = yf.download(ticker, start=start_date, progress=False)\n",
        "                        break\n",
        "                    except Exception as e:\n",
        "                        if \"rate limit\" in str(e).lower() or \"connection\" in str(e).lower():\n",
        "                            retry_count += 1\n",
        "                            if retry_count >= max_retries:\n",
        "                                raise Exception(f\"Max retries exceeded for {ticker}\")\n",
        "\n",
        "                            backoff_time *= 2  # Exponential backoff\n",
        "                            wait_time = backoff_time + random.uniform(0, 1)\n",
        "                            print(f\"  Rate limit hit. Retry {retry_count}/{max_retries}. Waiting {wait_time:.2f} seconds...\")\n",
        "                            time.sleep(wait_time)\n",
        "                        else:\n",
        "                            raise\n",
        "\n",
        "                # Save to cache\n",
        "                historical_data.to_csv(cache_file)\n",
        "                print(f\"  Saved data to cache: {cache_file}\")\n",
        "\n",
        "            if historical_data.empty:\n",
        "                print(f\"  No data available for {ticker}\")\n",
        "                continue\n",
        "\n",
        "            # Create month-day column for grouping\n",
        "            historical_data['month_day'] = historical_data.index.strftime('%m-%d')\n",
        "\n",
        "            # Calculate daily returns\n",
        "            if 'return' not in historical_data.columns:\n",
        "                historical_data['return'] = historical_data['Close'].pct_change()\n",
        "\n",
        "            # Group by month-day and analyze patterns\n",
        "            for month_day, group in historical_data.groupby('month_day'):\n",
        "                if len(group) >= years/2:  # Require at least half of the possible years to have data\n",
        "                    positive_returns = (group['return'] > 0).sum()\n",
        "                    total_returns = (~group['return'].isna()).sum()\n",
        "\n",
        "                    # If stock goes up on this date at least 80% of the time\n",
        "                    if total_returns > 0 and positive_returns / total_returns >= 0.8:\n",
        "                        if month_day not in up_patterns:\n",
        "                            up_patterns[month_day] = []\n",
        "                        up_patterns[month_day].append((ticker, positive_returns, total_returns))\n",
        "\n",
        "                    # If stock goes down on this date at least 80% of the time\n",
        "                    if total_returns > 0 and positive_returns / total_returns <= 0.2:\n",
        "                        if month_day not in down_patterns:\n",
        "                            down_patterns[month_day] = []\n",
        "                        down_patterns[month_day].append((ticker, total_returns - positive_returns, total_returns))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  Error analyzing {ticker}: {e}\")\n",
        "\n",
        "            # If we hit a rate limit or connection error, pause for longer\n",
        "            if \"rate limit\" in str(e).lower() or \"connection\" in str(e).lower():\n",
        "                pause_time = random.uniform(15.0, 30.0)\n",
        "                print(f\"  Rate limit or connection issue detected. Pausing for {pause_time:.2f} seconds...\")\n",
        "                time.sleep(pause_time)\n",
        "\n",
        "    return up_patterns, down_patterns\n",
        "\n",
        "def format_results(patterns, pattern_type):\n",
        "    \"\"\"\n",
        "    Format pattern results for display.\n",
        "\n",
        "    Parameters:\n",
        "    patterns (dict): Dictionary mapping dates to stocks with patterns\n",
        "    pattern_type (str): \"UP\" or \"DOWN\"\n",
        "\n",
        "    Returns:\n",
        "    DataFrame: Formatted results\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for date, stocks in patterns.items():\n",
        "        for stock, matches, total in stocks:\n",
        "            month, day = date.split('-')\n",
        "            current_year = datetime.datetime.now().year\n",
        "\n",
        "            # Create a list of dates for the past 10 years on which this pattern occurs\n",
        "            historical_dates = []\n",
        "            for year in range(current_year - 10, current_year + 1):\n",
        "                try:\n",
        "                    pattern_date = datetime.datetime(year, int(month), int(day))\n",
        "                    day_of_week = pattern_date.strftime('%A')\n",
        "\n",
        "                    # Check if it's a weekend\n",
        "                    is_weekend = day_of_week in ['Saturday', 'Sunday']\n",
        "\n",
        "                    historical_dates.append(f\"{pattern_date.strftime('%Y-%m-%d')} ({day_of_week}){' - Weekend' if is_weekend else ''}\")\n",
        "                except ValueError:\n",
        "                    # Skip invalid dates (e.g., February 29 in non-leap years)\n",
        "                    pass\n",
        "\n",
        "            results.append({\n",
        "                'Stock': stock,\n",
        "                'Pattern Date': date,\n",
        "                'Pattern Type': pattern_type,\n",
        "                'Success Rate': f\"{matches}/{total} ({(matches/total)*100:.1f}%)\",\n",
        "                'Historical Dates': ', '.join(historical_dates)\n",
        "            })\n",
        "\n",
        "    if results:\n",
        "        return pd.DataFrame(results)\n",
        "    else:\n",
        "        return pd.DataFrame(columns=['Stock', 'Pattern Date', 'Pattern Type', 'Success Rate', 'Historical Dates'])\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to run the stock date pattern screener.\n",
        "    \"\"\"\n",
        "    print(\"==== Stock Date Pattern Screener ====\")\n",
        "    print(\"This script uses caching to minimize API requests and avoid rate limiting.\")\n",
        "\n",
        "    # Allow for batch mode to process stocks in smaller batches\n",
        "    batch_size = 10  # Process 10 stocks at a time\n",
        "\n",
        "    print(\"\\nGetting top 100 S&P 500 stocks by market cap...\")\n",
        "    top_100_stocks = get_top_100_sp500_by_market_cap()\n",
        "    print(f\"Retrieved {len(top_100_stocks)} stocks.\")\n",
        "\n",
        "    # Process in batches to avoid rate limiting\n",
        "    all_up_patterns = {}\n",
        "    all_down_patterns = {}\n",
        "\n",
        "    for i in range(0, len(top_100_stocks), batch_size):\n",
        "        batch = top_100_stocks[i:i+batch_size]\n",
        "        print(f\"\\nProcessing batch {i//batch_size + 1}/{(len(top_100_stocks) + batch_size - 1)//batch_size}\")\n",
        "        print(f\"Analyzing date patterns for stocks {i+1}-{min(i+batch_size, len(top_100_stocks))} of {len(top_100_stocks)}...\")\n",
        "\n",
        "        up_patterns, down_patterns = analyze_date_patterns(batch)\n",
        "\n",
        "        # Merge results\n",
        "        for date, stocks in up_patterns.items():\n",
        "            if date not in all_up_patterns:\n",
        "                all_up_patterns[date] = []\n",
        "            all_up_patterns[date].extend(stocks)\n",
        "\n",
        "        for date, stocks in down_patterns.items():\n",
        "            if date not in all_down_patterns:\n",
        "                all_down_patterns[date] = []\n",
        "            all_down_patterns[date].extend(stocks)\n",
        "\n",
        "        # Save intermediate results\n",
        "        print(\"Saving intermediate results...\")\n",
        "        intermediate_up = format_results(all_up_patterns, \"UP\")\n",
        "        intermediate_down = format_results(all_down_patterns, \"DOWN\")\n",
        "\n",
        "        intermediate_results = pd.concat([intermediate_up, intermediate_down]).reset_index(drop=True)\n",
        "        if not intermediate_results.empty:\n",
        "            intermediate_results.to_csv(f\"stock_date_patterns_batch_{i//batch_size + 1}.csv\", index=False)\n",
        "\n",
        "        # Wait between batches to avoid rate limiting\n",
        "        if i + batch_size < len(top_100_stocks):\n",
        "            delay = random.uniform(5.0, 10.0)\n",
        "            print(f\"Waiting {delay:.2f} seconds before processing next batch...\")\n",
        "            time.sleep(delay)\n",
        "\n",
        "    print(\"\\nFormatting final results...\")\n",
        "    up_results = format_results(all_up_patterns, \"UP\")\n",
        "    down_results = format_results(all_down_patterns, \"DOWN\")\n",
        "\n",
        "    # Combine results\n",
        "    all_results = pd.concat([up_results, down_results]).reset_index(drop=True)\n",
        "\n",
        "    # Sort by pattern date\n",
        "    all_results = all_results.sort_values(['Pattern Date', 'Stock']).reset_index(drop=True)\n",
        "\n",
        "    print(\"\\nTop date patterns found:\")\n",
        "    if not all_results.empty:\n",
        "        # Print top 10 most consistent patterns\n",
        "        display_results = all_results.copy()\n",
        "        display_results['Success Rate Numeric'] = display_results['Success Rate'].apply(\n",
        "            lambda x: float(x.split('(')[1].split('%')[0])\n",
        "        )\n",
        "        display_results = display_results.sort_values('Success Rate Numeric', ascending=False).head(10)\n",
        "        display_results = display_results.drop('Success Rate Numeric', axis=1)\n",
        "        print(display_results)\n",
        "\n",
        "        # Save results to CSV\n",
        "        output_file = \"stock_date_patterns_final.csv\"\n",
        "        all_results.to_csv(output_file, index=False)\n",
        "        print(f\"\\nFull results saved to {output_file}\")\n",
        "    else:\n",
        "        print(\"No significant patterns found.\")\n",
        "\n",
        "    print(\"\\nAnalysis complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "X3GpNfOB6NyJ",
        "outputId": "e00fe76e-da13-4d4b-9179-a107b9317329"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== Stock Date Pattern Screener ====\n",
            "This script uses caching to minimize API requests and avoid rate limiting.\n",
            "\n",
            "Getting top 100 S&P 500 stocks by market cap...\n",
            "Loading top 100 stocks from cache file: sp500_top100_cache.csv\n",
            "Retrieved 100 stocks.\n",
            "\n",
            "Processing batch 1/10\n",
            "Analyzing date patterns for stocks 1-10 of 100...\n",
            "[1/10] Analyzing AAPL...\n",
            "  Loading cached data for AAPL\n",
            "  Error analyzing AAPL: 'Index' object has no attribute 'strftime'\n",
            "[2/10] Analyzing MSFT...\n",
            "  Loading cached data for MSFT\n",
            "  Error analyzing MSFT: 'Index' object has no attribute 'strftime'\n",
            "[3/10] Analyzing NVDA...\n",
            "  Loading cached data for NVDA\n",
            "  Error analyzing NVDA: 'Index' object has no attribute 'strftime'\n",
            "[4/10] Analyzing AMZN...\n",
            "  Loading cached data for AMZN\n",
            "  Error analyzing AMZN: 'Index' object has no attribute 'strftime'\n",
            "[5/10] Analyzing GOOG...\n",
            "  Loading cached data for GOOG\n",
            "  Error analyzing GOOG: 'Index' object has no attribute 'strftime'\n",
            "[6/10] Analyzing GOOGL...\n",
            "  Loading cached data for GOOGL\n",
            "  Error analyzing GOOGL: 'Index' object has no attribute 'strftime'\n",
            "[7/10] Analyzing META...\n",
            "  Loading cached data for META\n",
            "  Error analyzing META: 'Index' object has no attribute 'strftime'\n",
            "[8/10] Analyzing TSLA...\n",
            "  Loading cached data for TSLA\n",
            "  Error analyzing TSLA: 'Index' object has no attribute 'strftime'\n",
            "[9/10] Analyzing AVGO...\n",
            "  Loading cached data for AVGO\n",
            "  Error analyzing AVGO: 'Index' object has no attribute 'strftime'\n",
            "[10/10] Analyzing LLY...\n",
            "  Loading cached data for LLY\n",
            "  Error analyzing LLY: 'Index' object has no attribute 'strftime'\n",
            "Saving intermediate results...\n",
            "Waiting 8.89 seconds before processing next batch...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-b132009ac554>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-16-b132009ac554>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mdelay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Waiting {delay:.2f} seconds before processing next batch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nFormatting final results...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Backtesting"
      ],
      "metadata": {
        "id": "bH6Fi_9W9XiA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yp1bU9B_N7OC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}